{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d72e5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parshakova.tanya/opt/anaconda3/envs/routing/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import copy, time\n",
    "import random\n",
    "import pickle\n",
    "import scipy\n",
    "\n",
    "import mlrfit as mf\n",
    "from tqdm import tqdm\n",
    "\n",
    "import mfmodel as mfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4759ee57",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1001)\n",
    "random.seed(1001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b63e70b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_levels=10, num_sparsities=256\n",
      "(256, 9) [  0   1   3   7  15  31  63 127 255]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256/256 [00:00<00:00, 459.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_levels=10, num_sparsities=256\n",
      "(256, 9) [  0   1   3   7  15  31  63 127 255]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256/256 [00:00<00:00, 722.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_levels=10, num_sparsities=256\n",
      "(256, 9) [  0   1   3   7  15  31  63 127 255]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256/256 [00:00<00:00, 902.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_levels=10, num_sparsities=256\n",
      "(256, 9) [  0   1   3   7  15  31  63 127 255]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256/256 [00:00<00:00, 466.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_levels=10, num_sparsities=256\n",
      "(256, 9) [  0   1   3   7  15  31  63 127 255]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256/256 [00:00<00:00, 432.94it/s]\n"
     ]
    }
   ],
   "source": [
    "M = 5\n",
    "m = 300\n",
    "N = 50\n",
    "rank = 40\n",
    "\n",
    "# for _ in tqdm(range(M)):\n",
    "for _ in range(M):\n",
    "    # num_levels = int(np.ceil(np.log2(min(m, m))) + 1)\n",
    "    hpart = mf.random_hpartition(m,  m, level_list=None, symm=True)\n",
    "    ranks = mf.uniform_capped_ranks(rank, hpart)\n",
    "    hat_A = mf.MLRMatrix(hpart=hpart, ranks=ranks, debug=True)\n",
    "    hat_A.construct_sparse_format()\n",
    "    hat_A.B[:, -1] = np.abs(hat_A.B[:, -1]) + 1e-3\n",
    "    hat_A.C = hat_A.B\n",
    "    # hat_A.C[:, -1] = np.abs(hat_A.C[:, -1]) + 1e-3\n",
    "\n",
    "    part_sizes = []\n",
    "    for level in range(len(hat_A.hpart['rows']['lk'])):\n",
    "        part_sizes += [hat_A.hpart['rows']['lk'][level].size-1]\n",
    "\n",
    "    F0, D0 = hat_A.B[:, :-1], np.square(hat_A.B[:, -1:])\n",
    "    assert F0.shape[1] == rank-1 and np.allclose(np.concatenate([F0, np.sqrt(D0)], axis=1), hat_A.B)\n",
    "    D0 = D0[:, 0]\n",
    "    F_hpart = {\"lk\": hat_A.hpart['rows']['lk'][:-1], \"pi\":hat_A.hpart['rows']['pi']}\n",
    "\n",
    "    tilde_F0 = mf.convert_compressed_to_sparse(F0, F_hpart, ranks[:-1])\n",
    "    Sigma0 = mfm.perm_hat_Sigma(F0, D0, F_hpart, ranks) \n",
    "    assert np.allclose(Sigma0, tilde_F0 @ tilde_F0.T + np.diag(D0.flatten()))\n",
    "    assert np.allclose(Sigma0[hat_A.pi_inv_rows, :][:, hat_A.pi_inv_cols], hat_A.matrix())\n",
    "\n",
    "    row_selectors, si_groups, F_hpart = mfm.row_col_selections(hat_A.hpart)\n",
    "    num_sparsities = row_selectors.size - 1\n",
    "    lu, piv = scipy.linalg.lu_factor(Sigma0)\n",
    "    Y = np.random.randn(N, m)\n",
    "\n",
    "    F1 = mfm.EM_get_F(F0, lu, piv, Y, ranks, part_sizes, F_hpart, row_selectors, si_groups)\n",
    "    tilde_F1 = mf.convert_compressed_to_sparse(F1, F_hpart, ranks[:-1])\n",
    "    D_test = []\n",
    "    for si in tqdm(range(num_sparsities)):\n",
    "        r1, r2 = row_selectors[si: si+2]\n",
    "        si_col = mfm.group_to_indices(si_groups[si], part_sizes, ranks)\n",
    "        ri_F1_ciT = F1[r1:r2, :]\n",
    "        # test column selection in group_to_indices\n",
    "        assert np.allclose(ri_F1_ciT, tilde_F1[r1:r2, si_col].toarray())\n",
    "        ri_At_ci_t, ci_B_ci, r1, r2 = mfm.EM_intermediate_matrices(tilde_F0, Y, lu, piv, ranks, si, part_sizes, si_groups, row_selectors)\n",
    "\n",
    "        # computation of diagonal D\n",
    "        M1 = (1/N) * np.diag(Y[:, r1:r2].T @ Y[:, r1:r2] - 2 * ri_F1_ciT @ ri_At_ci_t + ri_F1_ciT @ ci_B_ci @ ri_F1_ciT.T)\n",
    "        M2 = (1/N) * ( np.einsum('ij,ji->i', Y[:, r1:r2].T, Y[:, r1:r2]) \\\n",
    "                            - 2 * np.einsum('ij,ji->i', ri_F1_ciT, ri_At_ci_t) \\\n",
    "                            + np.einsum('ij,jk,ki->i', ri_F1_ciT, ci_B_ci, ri_F1_ciT.T))\n",
    "        D_test += [M2]\n",
    "        assert np.allclose(M1, M2)\n",
    "        assert np.allclose(np.sum(M1 * np.power(D0[r1:r2], -1)), \n",
    "                           np.trace(np.diag(M1) @ np.diag(np.power(D0[r1:r2], -1))))\n",
    "    D_test = np.concatenate(D_test, axis=0)\n",
    "    D1 = mfm.EM_get_D(F0, F1, lu, piv, Y, ranks, part_sizes, F_hpart, row_selectors, si_groups)\n",
    "    assert np.allclose(D_test, D1)\n",
    "\n",
    "del tilde_F0, tilde_F1, hat_A, F0, F1, D1, D0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19c8a3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level=0, num_groups=1, mean_size=100.0\n",
      "level=1, num_groups=2, mean_size=50.0\n",
      "level=2, num_groups=100, mean_size=1.0\n",
      "num_levels=3, num_sparsities=2\n",
      "(2, 2) [0 1]\n"
     ]
    }
   ],
   "source": [
    "ranks = np.array([20, 10, 1])\n",
    "rank = ranks.sum()\n",
    "\n",
    "m = n = 100 \n",
    "N = 50\n",
    "L = ranks.size\n",
    "\n",
    "hpart = mf.random_hpartition(n, n, level_list=list(range(L-1)) + [int(np.ceil(np.log2(n)))], symm=True, perm=True)\n",
    "part_sizes = mfm.print_hpart_numgroups(hpart)\n",
    "row_selectors, si_groups, F_hpart = mfm.row_col_selections(hpart)\n",
    "\n",
    "Y = np.random.randn(N, m)\n",
    "block_diag_perm_Y = Y[:, hpart[\"rows\"][\"pi\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f4a0e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_sigma(F0, D0, hpart, ranks):\n",
    "    assert len(hpart[\"rows\"]['lk']) == 3\n",
    "    res = np.diag(D0) + 0\n",
    "    res += F0[:, :ranks[0]] @ F0[:, :ranks[0]].T\n",
    "    b1, b2, b3 = hpart[\"rows\"]['lk'][1]\n",
    "    res[b1:b2, b1:b2] += F0[b1:b2, ranks[0]:ranks[:2].sum()] @ F0[b1:b2, ranks[0]:ranks[:2].sum()].T\n",
    "    res[b2:b3, b2:b3] += F0[b2:b3, ranks[0]:ranks[:2].sum()] @ F0[b2:b3, ranks[0]:ranks[:2].sum()].T\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1574bb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "F0, D0 = np.random.randn(m, rank-1), np.square(np.random.rand(m)) + 1e-3\n",
    "\n",
    "F_init = F0 + 0.0\n",
    "D_init = D0 + 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4266c4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASSED\n"
     ]
    }
   ],
   "source": [
    "loglikelihoods = [-np.inf]\n",
    "for t in range(200):\n",
    "    Sigma0 = mfm.perm_hat_Sigma(F0, D0, F_hpart, ranks)\n",
    "    assert np.allclose(Sigma0, manual_sigma(F0, D0, hpart, ranks))\n",
    "\n",
    "    lu, piv = scipy.linalg.lu_factor(Sigma0)\n",
    "    loglikelihoods += [mfm.loglikelihood_value(Sigma0, lu, piv, block_diag_perm_Y)]\n",
    "\n",
    "    F1 = mfm.EM_get_F(F0, lu, piv, block_diag_perm_Y, ranks, part_sizes, F_hpart, row_selectors, si_groups)\n",
    "\n",
    "    # test computation of F1\n",
    "    tilde_F0 = mf.convert_compressed_to_sparse(F0, F_hpart, ranks[:-1])\n",
    "    s = tilde_F0.shape[1]\n",
    "    Sigma0_inv_F0 = scipy.linalg.lu_solve((lu, piv), tilde_F0.toarray())\n",
    "    Y_Sigma0_inv_F0 = block_diag_perm_Y @ Sigma0_inv_F0\n",
    "    A = Y_Sigma0_inv_F0.T @ block_diag_perm_Y\n",
    "    B = N * (np.eye(s) - Sigma0_inv_F0.T @ tilde_F0) + Y_Sigma0_inv_F0.T @ Y_Sigma0_inv_F0\n",
    "\n",
    "    F1_test = np.zeros((n, ranks[:-1].sum()))\n",
    "    b1, b2, b3 = hpart[\"rows\"]['lk'][1]\n",
    "    F1_test[:b2, :] = np.linalg.solve(B[:ranks[:2].sum(), :ranks[:2].sum()], A[:ranks[:2].sum(), :b2]).T\n",
    "    indices = np.concatenate([np.arange(ranks[0]), ranks[1] + np.arange(ranks[0], ranks[:2].sum())], axis=0)\n",
    "    F1_test[b2:, :] = np.linalg.solve(B[indices, :][:, indices], A[indices, :][:, b2:]).T\n",
    "\n",
    "    assert np.allclose(F1, F1_test)\n",
    "    D1 = mfm.EM_get_D(F0, F1, lu, piv, block_diag_perm_Y, ranks, part_sizes, F_hpart, row_selectors, si_groups)\n",
    "    assert D1.min() >= -1e-8 and loglikelihoods[-2] - 1e-8 <= loglikelihoods[-1], print(loglikelihoods[-5:])\n",
    "    F0, D0 = F1, D1\n",
    "\n",
    "print(\"PASSED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0edd8e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASSED\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    A, B = np.random.randn(20, 20), np.random.randn(20, 20)\n",
    "    assert np.allclose(np.einsum('ij,ji->i', A, B), np.diag(A @ B))\n",
    "\n",
    "print(\"PASSED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f680a4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "cbb24b48ae2642bb0fe3c3a73dd180d2cf6b4e8df7de3f3850c06f8dd008ecd7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
